{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import config\n",
    "import codecs\n",
    "import chardet\n",
    "import string\n",
    "import freeling\n",
    "\n",
    "#Freeling initialize\n",
    "\n",
    "FREELINGDIR = \"/usr/local\"\n",
    "DATA = FREELINGDIR + \"/share/freeling/\"\n",
    "LANG=\"es\"\n",
    "\n",
    "freeling.util_init_locale(\"default\")\n",
    "\n",
    "# create language analyzer\n",
    "la = freeling.lang_ident(DATA + \"common/lang_ident/ident.dat\")\n",
    "# create options set for maco analyzer. Default values are Ok, except for data files.\n",
    "op = freeling.maco_options(\"es\")\n",
    "op.set_data_files(\"\",\n",
    "                  DATA + \"common/punct.dat\",\n",
    "                  DATA + LANG + \"/dicc.src\",\n",
    "                  DATA + LANG + \"/afixos.dat\",\n",
    "                  \"\",\n",
    "                  DATA + LANG + \"/locucions.dat\",\n",
    "                  DATA + LANG + \"/np.dat\",\n",
    "                  DATA + LANG + \"/quantities.dat\",\n",
    "                  DATA + LANG + \"/probabilitats.dat\")\n",
    "\n",
    "\n",
    "# create analyzers\n",
    "tk=freeling.tokenizer(DATA+LANG+\"/tokenizer.dat\")\n",
    "sp=freeling.splitter(DATA+LANG+\"/splitter.dat\")\n",
    "sid=sp.open_session()\n",
    "mf=freeling.maco(op)\n",
    "\n",
    "# activate mmorpho odules to be used in next call\n",
    "mf.set_active_options(False, True, True, True,  # select which among created \n",
    "                      True, True, False, True,  # submodules are to be used. \n",
    "                      True, True, True, True ) # default: all created submodules are used\n",
    "\n",
    "# create tagger, sense anotator, and parsers\n",
    "tg=freeling.hmm_tagger(DATA+LANG+\"/tagger.dat\",True,2)\n",
    "sen=freeling.senses(DATA+LANG+\"/senses.dat\")\n",
    "parser= freeling.chart_parser(DATA+LANG+\"/chunker/grammar-chunk.dat\")\n",
    "dep=freeling.dep_txala(DATA+LANG+\"/dep_txala/dependences.dat\", parser.get_start_symbol())\n",
    "\n",
    "def generate_freeling_annotations(input_text):\n",
    "    tokens = tk.tokenize(input_text)\n",
    "    ls = sp.split(sid,tokens,False)\n",
    "\n",
    "    ls = mf.analyze(ls)\n",
    "    ls = tg.analyze(ls)\n",
    "    ls = sen.analyze(ls)\n",
    "    ls = parser.analyze(ls)\n",
    "    ls = dep.analyze(ls)\n",
    "    \n",
    "    return ls\n",
    "\n",
    "def process_line(input_line):\n",
    "    result = []\n",
    "    freeling_annotations = generate_freeling_annotations(input_line)\n",
    "    \n",
    "    for s in freeling_annotations:\n",
    "        ws = s.get_words()\n",
    "        for w in ws:\n",
    "            result.append([w.get_form(), w.get_lemma(),w.get_tag()])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_line_filtered(input_line, filters):\n",
    "    result = []\n",
    "    freeling_annotations = generate_freeling_annotations(input_line)\n",
    "    \n",
    "    for s in freeling_annotations:\n",
    "        ws = s.get_words()\n",
    "        for w in ws:\n",
    "            if pass_filter(w.get_tag(), filters):\n",
    "                result.append([w.get_form(), w.get_lemma(),w.get_tag()])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def pass_filter(word, filters):\n",
    "    for f in filters:\n",
    "        if word.startswith(f):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_lemmatized_text(input_line, filters):\n",
    "    result = \"\"\n",
    "    tuples  = process_line_filtered(input_line, filters)\n",
    "    for t in tuples:\n",
    "        result = result + str(\" \") + str(t[1])\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def parse_file(file_path):\n",
    "    dic = {}\n",
    "    f= codecs.open(file_path, encoding='utf-8')\n",
    "    u = f.read()\n",
    "    line =u.split('\\n')\n",
    "    dic['titulo'] = line[0]\n",
    "    dic['resumen'] = line[1]\n",
    "    dic['texto'] = line[2]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents_lemmatized = []\n",
    "\n",
    "for file_name in os.listdir(config.DATASET_TED_RAW):\n",
    "    try:\n",
    "        documents.append(parse_file(config.DATASET_TED_RAW + '/' + file_name))\n",
    "    except Exception as e:\n",
    "        print file_name\n",
    "        print e.message\n",
    "\n",
    "for doc in documents:\n",
    "    documents_lemmatized.append(get_lemmatized_text(str(doc['texto']).encode('utf-8'), [\"N\", \"V\", \"AQ\"]))\n",
    "\n",
    "print documents_lemmatized[0]\n",
    "print documents_lemmatized[1]\n",
    "print documents_lemmatized[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
